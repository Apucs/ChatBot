import trax
from trax import layers as tl
from config import *
from models import Model
import numpy as np
import pickle
import argparse
from termcolor import colored


parser = argparse.ArgumentParser()
parser.add_argument('--s_sen', help="sentence to start the conversation", required=True, type=str)
parser.add_argument('--max_len', help="Maximum length of the conversation", default= 120, type=int)
args = parser.parse_args()

#print("Opening statement is:" + args.s_sen)



def tokenize(sentence, vocab_file, vocab_dir):
    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]

def detokenize(tokens, vocab_file, vocab_dir):
    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)


def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):
    """
    Args:
        ReformerLM:  the Reformer language model you just trained
        start_sentence (string): starting sentence of the conversation
        vocab_file (string): vocabulary filename
        vocab_dir (string): directory of the vocabulary file
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)

    Returns:
        generator: yields the next symbol generated by the model
    """

    # Create input tokens using the the tokenize function
    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)

    # Add batch dimension to array. Convert from (n,) to (x, n) where
    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)
    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)

    # call the autoregressive_sample_stream function from trax
    output_gen = trax.supervised.decoding.autoregressive_sample_stream(
        # model
        model=ReformerLM,
        # inputs will be the tokens with batch dimension
        inputs=input_tokens_with_batch,
        # temperature
        temperature=temperature
    )
    return output_gen

def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):
    """
    Args:
        ReformerLM:  the Reformer language model you just trained
        model_state (np.array): initial state of the model before decoding
        start_sentence (string): starting sentence of the conversation
        vocab_file (string): vocabulary filename
        vocab_dir (string): directory of the vocabulary file
        max_len (int): maximum number of tokens to generate 
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)

    Returns:
        generator: yields the next symbol generated by the model
    """  
    
    # define the delimiters we used during training
    delimiter_1 = 'Person 1: ' 
    delimiter_2 = 'Person 2: '
    
    # initialize detokenized output
    sentence = ''
    
    # token counter
    counter = 0
    
    # output tokens. we insert a ': ' for formatting
    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]
    
    # reset the model state when starting a new dialogue
    ReformerLM.state = model_state
    
    # calls the output generator implemented earlier
    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, 
                                   vocab_dir=VOCAB_DIR, temperature=temperature)
    
    # print the starting sentence
    print('\n')
    print(colored(start_sentence.split(delimiter_2)[0].strip(), 'green'))
    
    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.
    for o in output:
        
        result.append(o)
        
        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)
        
        if sentence.endswith(delimiter_1):
            sentence = sentence.split(delimiter_1)[0]
            print(colored(f'{delimiter_2}{sentence}', 'red'))
            sentence = ''
            result.clear()
        
        elif sentence.endswith(delimiter_2):
            sentence = sentence.split(delimiter_2)[0]
            print(colored(f'{delimiter_1}{sentence}', 'green'))
            sentence = ''
            result.clear()

        counter += 1
        
        if counter > max_len:
            break 



# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention
def attention(*args, **kwargs):
    # number of input positions to remember in a cache when doing fast inference.
    kwargs['predict_mem_len'] = 120
    # number of input elements to drop once the fast inference input cache fills up.
    kwargs['predict_drop_len'] = 120
    # return the attention layer with the parameters defined above
    return tl.SelfAttention(*args, **kwargs)



WEIGHTS_FROM_FILE = ()

with open('weights', 'rb') as file:
    WEIGHTS_FROM_FILE = pickle.load(file)

shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)

# test_model = Model.ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)

# test_output_gen = ReformerLM_output_gen(test_model, "test", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)

# test_model.init_weights_and_state(shape11)

# test_model.weights = WEIGHTS_FROM_FILE

# output = []

# for i in range(6):
#     output.append(next(test_output_gen)[0])

# print(output)

# # free memory
# del test_model
# del WEIGHTS_FROM_FILE
# del test_output_gen


model = Model.ReformerLM(
    vocab_size=33000,
    n_layers=6,
    mode='predict',
    attention_type=attention,
)

model.init_from_file('chatbot_model1.pkl.gz',
                     weights_only=True, input_signature=shape11)

STARTING_STATE = model.state

   


#sample_sentence = ' Person 1: Are there theatres in town? Person 2: '

s_sentence = args.s_sen
max_len = args.max_len
print("Starting sentence:", s_sentence)
print("Maximum length of the sentence:", max_len)
generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=s_sentence, 
                  vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=max_len, temperature=0.2)

print('\n')


# sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '
# generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, 
#                   vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)
# print('\n')

# sample_sentence = ' Person 1: Can you book a taxi? Person 2: '
# generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, 
#                   vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)
# print('\n')
